{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph Advanced Concepts: Middleware & Human-in-the-Loop\n",
    "\n",
    "Welcome to LangGraph Advanced Concepts! This notebook builds on the foundations from LangGraph 101 and introduces two powerful patterns for production agents.\n",
    "\n",
    "**What you'll learn:**\n",
    "- **Human-in-the-Loop** - Pause agents for human review and approval\n",
    "- **Middleware** - Modify agent behavior at key points in execution\n",
    "- **Tool Review** - Add approval workflows to sensitive tools\n",
    "- **Dynamic Behavior** - Adapt agent responses based on context\n",
    "\n",
    "**Prerequisites:** Complete `langgraph_101.ipynb` \n",
    "</br>\n",
    "</br>\n",
    "\n",
    "---\n",
    "</br>\n",
    "\n",
    "> **Note:** These patterns are essential for production agents where safety, compliance, and user control are critical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's quickly set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "load_dotenv(dotenv_path=\"../.env\", override=True)\n",
    "\n",
    "# Initialize model\n",
    "model = init_chat_model(\"openai:gpt-5-nano\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Human-in-the-Loop with Interrupts\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Imagine you're building an agent that can send emails or make purchases. You don't want it to take these actions automatically - you want human approval first!\n",
    "\n",
    "**Human-in-the-loop** lets you:\n",
    "- Pause execution for review\n",
    "- Approve, reject, or edit actions\n",
    "- Add safety controls to sensitive operations\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. Agent encounters an `interrupt()` - execution pauses\n",
    "2. System surfaces information to human\n",
    "3. Human provides input (approve/reject/edit)\n",
    "4. Agent resumes with `Command(resume=...)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Simple Approval Workflow\n",
    "\n",
    "Let's start with a simple example - asking for approval before sending an email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool created successfully!\n",
      "Tool name: send_email\n",
      "Tool description: Send an email to a recipient.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.types import interrupt\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def send_email(to: str, subject: str, body: str) -> str:\n",
    "    \"\"\"Send an email to a recipient.\"\"\"\n",
    "    \n",
    "    # Pause for human approval\n",
    "    approval = interrupt({\n",
    "        \"action\": \"send_email\",\n",
    "        \"to\": to,\n",
    "        \"subject\": subject,\n",
    "        \"body\": body,\n",
    "        \"message\": \"Do you want to send this email?\"\n",
    "    })\n",
    "    \n",
    "    if approval.get(\"approved\"): # Will be true if accepted, false if declined\n",
    "        # In production, this would actually send the email\n",
    "        return f\" Email sent to {to} with subject '{subject}'\"\n",
    "    else:\n",
    "        return \"Email cancelled by user\"\n",
    "\n",
    "# Test the tool directly\n",
    "print(\"Tool created successfully!\")\n",
    "print(f\"Tool name: {send_email.name}\")\n",
    "print(f\"Tool description: {send_email.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an Agent with Human-in-the-Loop\n",
    "\n",
    "Now let's create an agent that uses this tool. **Remember:** Interrupts require a checkpointer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "import uuid\n",
    "\n",
    "# Create checkpointer for persistence\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "# Create agent with the email tool\n",
    "agent = create_agent(\n",
    "    model=\"openai:gpt-5-nano\",\n",
    "    tools=[send_email],\n",
    "    system_prompt=\"You are a helpful email assistant. When asked to send emails, use the send_email tool.\",\n",
    "    checkpointer=checkpointer  # Required for interrupts\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Until Interrupt\n",
    "\n",
    "Let's run the agent and see it pause for approval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent paused for approval\n",
      "\n",
      "Interrupt details:\n",
      "  To: alice@example.com\n",
      "  Subject: Meeting Tomorrow\n",
      "  Body: Let's meet at 3pm.\n",
      "  Message: Do you want to send this email?\n"
     ]
    }
   ],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "# Create a unique thread for this conversation\n",
    "thread_id = str(uuid.uuid4())\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "# Run the agent and see it pause for approval\n",
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"Send an email to alice@example.com with subject 'Meeting Tomorrow' and body 'Let's meet at 3pm.'\")]\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Check if we hit an interrupt\n",
    "\n",
    "if \"__interrupt__\" in result:\n",
    "    print(\"Agent paused for approval\\n\")\n",
    "\n",
    "    interrupt_info = result[\"__interrupt__\"][0]\n",
    "\n",
    "    print(\"Interrupt details:\")\n",
    "    print(f\"  To: {interrupt_info.value['to']}\")\n",
    "    print(f\"  Subject: {interrupt_info.value['subject']}\")\n",
    "    print(f\"  Body: {interrupt_info.value['body']}\")\n",
    "    print(f\"  Message: {interrupt_info.value['message']}\")\n",
    "else:\n",
    "    print(\"Agent completed without interrupt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resuming with Approval\n",
    "\n",
    "Now let's approve the email and let the agent continue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final response:\n",
      "Done. The email has been sent to alice@example.com with subject \"Meeting Tomorrow\" and body \"Let's meet at 3pm.\" Would you like me to set a reminder or send a follow-up?\n"
     ]
    }
   ],
   "source": [
    "from langgraph.types import Command\n",
    "\n",
    "# Resume with approval\n",
    "result = agent.invoke(\n",
    "    Command(resume={\"approved\": True}),\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Print the final response\n",
    "print(\"Final response:\")\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Try Rejecting the Email\n",
    "\n",
    "Run the cells again, but this time reject the email by passing `{\"approved\": False}`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final response:\n",
      "The previous send attempt was canceled. Would you like me to retry sending the email with the same details (to bob@example.com, subject \"Hello!\", body \"Hello!\")? Or would you prefer a different subject or body?\n"
     ]
    }
   ],
   "source": [
    "# New thread for rejection example\n",
    "thread_id_2 = str(uuid.uuid4())\n",
    "config_2 = {\"configurable\": {\"thread_id\": thread_id_2}}\n",
    "\n",
    "# Run until interrupt\n",
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"Send an email to bob@example.com saying 'Hello!'\")]\n",
    "    },\n",
    "    config=config_2\n",
    ")\n",
    "\n",
    "# Resume with rejection\n",
    "result = agent.invoke(\n",
    "    Command(resume={\"approved\": False}),  # Reject the email\n",
    "    config=config_2\n",
    ")\n",
    "\n",
    "print(\"Final response:\")\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Advanced Pattern - Edit Before Execution\n",
    "\n",
    "Sometimes you want to **edit** the tool call, not just approve/reject it. Let's enhance our tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def send_email_v2(to: str, subject: str, body: str) -> str:\n",
    "    \"\"\"Send an email to a recipient.\"\"\"\n",
    "    \n",
    "    # Pause for human review\n",
    "    response = interrupt({\n",
    "        \"action\": \"send_email\",\n",
    "        \"to\": to,\n",
    "        \"subject\": subject,\n",
    "        \"body\": body,\n",
    "        \"message\": \"Review this email. You can approve, reject, or edit it.\"\n",
    "    })\n",
    "    \n",
    "    # Handle different response types\n",
    "    if response[\"type\"] == \"approve\":\n",
    "        return f\"Email sent to {to} with subject '{subject}'\"\n",
    "\n",
    "    elif response[\"type\"] == \"reject\":\n",
    "        return \"Email cancelled\"\n",
    "\n",
    "    elif response[\"type\"] == \"edit\":\n",
    "        # Use edited values\n",
    "        to = response.get(\"to\", to)\n",
    "        subject = response.get(\"subject\", subject)\n",
    "        body = response.get(\"body\", body)\n",
    "        return f\"\"\"Email sent with edits:\n",
    "                To: {to}\n",
    "                Subject: {subject}\n",
    "                Body: {body}\"\"\"\n",
    "    \n",
    "    return \"Unknown response\"\n",
    "\n",
    "# Create new agent with enhanced tool\n",
    "agent_v2 = create_agent(\n",
    "    model=\"openai:gpt-5-nano\",\n",
    "    tools=[send_email_v2],\n",
    "    system_prompt=\"You are a helpful email assistant.\",\n",
    "    checkpointer=MemorySaver()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paused for review...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run and edit the email\n",
    "thread_id_3 = str(uuid.uuid4())\n",
    "config_3 = {\"configurable\": {\"thread_id\": thread_id_3}}\n",
    "\n",
    "# Run until interrupt\n",
    "result = agent_v2.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"Send an email to team@example.com about the meeting\")]\n",
    "    },\n",
    "    config=config_3\n",
    ")\n",
    "\n",
    "print(\"Paused for review...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets edit the email subject to make it URGENT meeting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final response:\n",
      "I can send that, but I need a couple of details to finalize the subject and body. Here are two ready-to-send templates you can choose from or modify. Tell me which one you prefer and provide the missing details (date, time, link/location, agenda, and your name), and I’ll send it right away.\n",
      "\n",
      "Option A — Short reminder\n",
      "Subject: Meeting Reminder\n",
      "Body:\n",
      "Hi team,\n",
      "This is a reminder about our upcoming meeting.\n",
      "Date: [DATE]\n",
      "Time: [TIME]\n",
      "Location/Link: [LOCATION]\n",
      "Please come prepared to discuss [AGENDA].\n",
      "Best regards,\n",
      "[Your Name]\n",
      "\n",
      "Option B — Invite with agenda\n",
      "Subject: Invitation: Team Meeting on [DATE] at [TIME]\n",
      "Body:\n",
      "Hi team,\n",
      "You’re invited to our team meeting on [DATE] at [TIME].\n",
      "Location/Call: [LINK/ROOM]\n",
      "Agenda: [AGENDA]\n",
      "If you’re unable to attend, please let me know.\n",
      "Best regards,\n",
      "[Your Name]\n",
      "\n",
      "If you’d like, I can also craft a version with more details or tailor it for a particular channel (e.g., calendar invite, Slack notice, etc.). Please provide:\n",
      "- Date\n",
      "- Time\n",
      "- Location or video call link\n",
      "- Agenda or key discussion points\n",
      "- Your name or signature line\n",
      "\n",
      "Or tell me to proceed with one of the templates above using the details you provide.\n"
     ]
    }
   ],
   "source": [
    "# Resume with edits\n",
    "result = agent_v2.invoke(\n",
    "    Command(resume={\n",
    "        \"type\": \"edit\",\n",
    "        \"subject\": \"URGENT: Meeting Today at 2pm\",  # We have edited the email subject\n",
    "        \"body\": \"This is the edited email body with more details.\"\n",
    "    }),\n",
    "    config=config_3\n",
    ")\n",
    "\n",
    "print(\"Final response:\")\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Introduction to Middleware\n",
    "\n",
    "**Middleware** provides fine-grained control over the agent loop. It lets you:\n",
    "- Inspect state before/after model calls\n",
    "- Modify model requests dynamically\n",
    "- Add custom logic at key execution points\n",
    "\n",
    "### The Agent Loop\n",
    "\n",
    "```\n",
    "Input --> [before_model] --> [wrap_model_call] --> Model --> [after_model] --> Tools --> ...\n",
    "```\n",
    "\n",
    "Middleware hooks into this loop:\n",
    "- **`before_model`** - Runs before model execution, can update state\n",
    "- **`wrap_model_call`** - Wraps the model call, control when/how the model is invoked\n",
    "- **`after_model`** - Runs after model execution, before tools\n",
    "\n",
    "### Two Hook Styles\n",
    "\n",
    "**Node-style hooks** run sequentially:\n",
    "- `before_agent`, `before_model`, `after_model`, `after_agent`\n",
    "- Good for logging, validation, state updates\n",
    "\n",
    "**Wrap-style hooks** intercept execution:\n",
    "- `wrap_model_call`, `wrap_tool_call`\n",
    "- Full control over handler calls\n",
    "- Good for retries, caching, transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Dynamic System Prompt\n",
    "\n",
    "Let's create middleware that changes the system prompt based on the user's role:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "from typing import TypedDict\n",
    "\n",
    "# Define context schema\n",
    "class Context(TypedDict):\n",
    "    user_role: str\n",
    "\n",
    "# Create middleware using decorator\n",
    "@dynamic_prompt\n",
    "def dynamic_prompt_middleware(request: ModelRequest) -> str:\n",
    "    \"\"\"Adjust system prompt based on user role.\"\"\"\n",
    "    \n",
    "    user_role = request.runtime.context.get(\"user_role\", \"general\")\n",
    "    \n",
    "    if user_role == \"expert\":\n",
    "        return \"You are an AI assistant for experts. Provide detailed technical responses with code examples.\"\n",
    "    elif user_role == \"beginner\":\n",
    "        return \"You are an AI assistant for beginners. Explain concepts simply, avoid jargon.\"\n",
    "    else:\n",
    "        return \"You are a helpful AI assistant.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def explain_concept(concept: str) -> str:\n",
    "    \"\"\"Explain a programming concept.\"\"\"\n",
    "    explanations = {\n",
    "        \"async\": \"Asynchronous programming allows code to run without blocking.\",\n",
    "        \"recursion\": \"Recursion is when a function calls itself.\"\n",
    "    }\n",
    "    return explanations.get(concept.lower(), \"Concept not found.\")\n",
    "\n",
    "# Create agent with middleware\n",
    "agent_with_middleware = create_agent(\n",
    "    model=\"openai:gpt-5-nano\",\n",
    "    tools=[explain_concept],\n",
    "    middleware=[dynamic_prompt_middleware],\n",
    "    context_schema=Context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Different User Roles\n",
    "\n",
    "Let's see how the agent responds differently based on user role:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "EXPERT USER\n",
      "==================================================\n",
      "Async programming is a way to handle operations that take time (like disk I/O, network requests, or user input) without blocking the main thread. The goal is to keep your program responsive and scalable by overlapping work instead of waiting in a single thread.\n",
      "\n",
      "Key concepts\n",
      "\n",
      "- Blocking vs non-blocking I/O\n",
      "  - Blocking: a call waits (blocks) until the operation finishes.\n",
      "  - Non-blocking: the operation starts and returns immediately; you get notified (via callbacks, promises/futures, or await) when it completes.\n",
      "\n",
      "- Concurrency vs parallelism\n",
      "  - Concurrency: many tasks make progress over time, potentially interleaved on a single thread (e.g., an event loop).\n",
      "  - Parallelism: multiple tasks actually run at the same time, typically on multiple cores or processes.\n",
      "\n",
      "- Event loop vs threads\n",
      "  - Event loop (asynchronous model): a single-threaded loop that schedules and runs many small tasks, often using non-blocking I/O.\n",
      "  - Threads: many OS threads can run in parallel; async code often uses a small pool of worker threads for CPU-bound work or to block-safe boundaries.\n",
      "\n",
      "- Futures/Promises and async/await\n",
      "  - Futures/Promises: placeholders for results of asynchronous operations.\n",
      "  - Async/await: syntactic sugar to write asynchronous code in a style that looks synchronous, improving readability and error handling.\n",
      "\n",
      "- Coroutines and schedulers\n",
      "  - Coroutines are lightweight units of work that can be suspended and resumed.\n",
      "  - A runtime (event loop or thread pool) schedules these coroutines.\n",
      "\n",
      "- Cancellation, timeouts, and error handling\n",
      "  - You typically want a way to cancel in-flight operations, impose timeouts, and propagate errors cleanly.\n",
      "\n",
      "Common patterns\n",
      "\n",
      "- Sequential vs parallel composition\n",
      "  - Sequential: one await depends on the previous result.\n",
      "  - Parallel: start multiple operations, then await their results together (e.g., Promise.all in JS, asyncio.gather in Python, Task.WhenAll in C#).\n",
      "\n",
      "- Streaming and back-pressure\n",
      "  - Large data or continuous input/output can be processed in chunks rather than loaded fully into memory.\n",
      "\n",
      "- CPU-bound vs I/O-bound\n",
      "  - Async is most beneficial for I/O-bound workloads. CPU-bound work can block the event loop; you usually offload it to worker threads or processes.\n",
      "\n",
      "Code examples\n",
      "\n",
      "JavaScript (Node.js) with async/await and Promise.all\n",
      "\n",
      "- Assume Node.js 18+ or environments with global fetch available. You can also use node-fetch or axios; here we use fetch-style API.\n",
      "\n",
      "- Sequential version (for contrast)\n",
      "\n",
      "async function fetchUrl(url) {\n",
      "  const res = await fetch(url);\n",
      "  return res.text();\n",
      "}\n",
      "\n",
      "async function fetchSequential(urls) {\n",
      "  const results = [];\n",
      "  for (const url of urls) {\n",
      "    results.push(await fetchUrl(url)); // waits for each one\n",
      "  }\n",
      "  return results;\n",
      "}\n",
      "\n",
      "- Parallel version using Promise.all\n",
      "\n",
      "async function fetchUrl(url) {\n",
      "  const res = await fetch(url);\n",
      "  return res.text();\n",
      "}\n",
      "\n",
      "async function fetchParallel(urls) {\n",
      "  const promises = urls.map(url => fetchUrl(url));\n",
      "  const results = await Promise.all(promises);\n",
      "  return results;\n",
      "}\n",
      "\n",
      "// Usage\n",
      "const urls = [\n",
      "  'https://example.com/a',\n",
      "  'https://example.com/b',\n",
      "  'https://example.com/c'\n",
      "];\n",
      "\n",
      "fetchParallel(urls)\n",
      "  .then(results => {\n",
      "    console.log('All fetched:', results.length);\n",
      "  })\n",
      "  .catch(err => {\n",
      "    console.error('One or more fetches failed:', err);\n",
      "  });\n",
      "\n",
      "- Abort / cancellation with AbortController (for fetch)\n",
      "\n",
      "async function fetchWithCancellation(urls) {\n",
      "  const controller = new AbortController();\n",
      "  const signals = urls.map(() => controller.signal);\n",
      "\n",
      "  // Start all fetches with the same controller (cancel when needed)\n",
      "  const promises = urls.map(url => fetch(url, { signal: controller.signal }).then(r => r.text()));\n",
      "\n",
      "  // Cancel after a timeout\n",
      "  const timeout = setTimeout(() => controller.abort(), 5000);\n",
      "\n",
      "  try {\n",
      "    const results = await Promise.all(promises);\n",
      "    clearTimeout(timeout);\n",
      "    return results;\n",
      "  } catch (err) {\n",
      "    if (controller.signal.aborted) {\n",
      "      throw new Error('Fetch cancelled');\n",
      "    }\n",
      "    throw err;\n",
      "  }\n",
      "}\n",
      "\n",
      "Python asyncio with aiohttp\n",
      "\n",
      "- Basic concurrent HTTP requests\n",
      "\n",
      "import asyncio\n",
      "import aiohttp\n",
      "\n",
      "async def fetch(session, url):\n",
      "    async with session.get(url) as resp:\n",
      "        text = await resp.text()\n",
      "        return url, text[:100]  # grab a preview\n",
      "\n",
      "async def main():\n",
      "    urls = [\n",
      "        'https://example.com',\n",
      "        'https://example.org',\n",
      "        'https://example.net',\n",
      "    ]\n",
      "    async with aiohttp.ClientSession() as session:\n",
      "        tasks = [fetch(session, u) for u in urls]\n",
      "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
      "        for r in results:\n",
      "            if isinstance(r, Exception):\n",
      "                print('Error:', r)\n",
      "            else:\n",
      "                url, preview = r\n",
      "                print(f'{url} -> {preview}')\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    asyncio.run(main())\n",
      "\n",
      "- Timeouts and cancellation\n",
      "\n",
      "async def fetch_with_timeout(session, url, timeout_sec=5.0):\n",
      "    try:\n",
      "        async with asyncio.timeout(timeout_sec):\n",
      "            async with session.get(url) as resp:\n",
      "                return await resp.text()\n",
      "    except asyncio.TimeoutError:\n",
      "        print(f'Timeout fetching {url}')\n",
      "        return None\n",
      "\n",
      "- Parallel + error handling\n",
      "\n",
      "async def main():\n",
      "    urls = ['https://example.com', 'https://invalid.url']\n",
      "    async with aiohttp.ClientSession() as session:\n",
      "        tasks = [fetch(session, u) for u in urls]\n",
      "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
      "        for r in results:\n",
      "            if isinstance(r, Exception):\n",
      "                print('Error:', r)\n",
      "\n",
      "C# async/await with HttpClient\n",
      "\n",
      "using System;\n",
      "using System.Net.Http;\n",
      "using System.Threading.Tasks;\n",
      "\n",
      "class AsyncDemo\n",
      "{\n",
      "    private static HttpClient client = new HttpClient();\n",
      "\n",
      "    static async Task<string> DownloadAsync(string url)\n",
      "    {\n",
      "        // GetStringAsync is asynchronous but uses I/O-bound wait\n",
      "        return await client.GetStringAsync(url);\n",
      "    }\n",
      "\n",
      "    static async Task Main(string[] args)\n",
      "    {\n",
      "        string[] urls = {\n",
      "            \"https://example.com\",\n",
      "            \"https://example.org\",\n",
      "            \"https://example.net\"\n",
      "        };\n",
      "\n",
      "        // Run in parallel\n",
      "        var tasks = Array.ConvertAll(urls, u => DownloadAsync(u));\n",
      "        try\n",
      "        {\n",
      "            string[] results = await Task.WhenAll(tasks);\n",
      "            foreach (var r in results)\n",
      "            {\n",
      "                Console.WriteLine($\"Downloaded {r.Length} chars\");\n",
      "            }\n",
      "        }\n",
      "        catch (Exception ex)\n",
      "        {\n",
      "            Console.WriteLine(\"One or more requests failed: \" + ex);\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "Patterns and pitfalls (quick notes)\n",
      "\n",
      "- Avoid blocking the event loop:\n",
      "  - In JavaScript, don't run CPU-heavy work on the main thread; use worker threads if needed.\n",
      "  In Python, avoid blocking the event loop with sync I/O or CPU-heavy work; offload CPU work to a ThreadPoolExecutor or ProcessPoolExecutor.\n",
      "\n",
      "- Proper composition:\n",
      "  - Use Promise.all / asyncio.gather for parallelism when tasks are independent.\n",
      "  - Use sequential awaits when tasks depend on each other.\n",
      "\n",
      "- Error handling:\n",
      "  - In JS, catch inside async functions and also handle Promise rejection.\n",
      "  - In Python, use try/except around await expressions; asyncio.gather with return_exceptions=True to capture per-task errors.\n",
      "  - In C#, catch AggregateException semantics when awaiting tasks created with Task.Run, though Task.WhenAll unwraps to a single exception type.\n",
      "\n",
      "- Cancellation:\n",
      "  - Provide a cancellation mechanism (AbortController in JS, CancellationToken in .NET, asyncio timeout/ cancellation in Python) to stop in-flight work gracefully.\n",
      "\n",
      "- Testing async code:\n",
      "  - JavaScript: tests can return promises or use async test functions.\n",
      "  - Python: pytest-asyncio allows writing async tests.\n",
      "  - C#: xUnit/NUnit support async tests returning Task.\n",
      "\n",
      "When to use async programming\n",
      "\n",
      "- Use async when you have I/O-bound tasks that would otherwise block:\n",
      "  - Web requests, file or network I/O, database calls, timers, user input handling.\n",
      "- Do not rely on async to speed up CPU-bound work. For CPU-bound tasks, consider:\n",
      "  - Offloading to worker threads/processes (e.g., Node worker threads, Python multiprocessing, C# Task.Run with CPU-bound work).\n",
      "\n",
      "Common misconceptions\n",
      "\n",
      "- Async does not automatically make code faster. It improves responsiveness and throughput for I/O-bound workloads by overlapping work, not by making a single operation complete faster.\n",
      "- Async does not remove the need for error handling. Errors can occur at the same stages; you must propagate and handle them properly.\n",
      "\n",
      "A quick comparison cheat sheet\n",
      "\n",
      "- JavaScript: async/await with Promises; single-threaded event loop; non-blocking I/O.\n",
      "- Python: asyncio with async/await; coroutines; event loop; awaits I/O-based tasks.\n",
      "- C#: async/await with Tasks; typically uses thread pool for I/O completion ports; can offload CPU work with Task.Run.\n",
      "\n",
      "If you want, tell me your target language or environment (Node.js, Python, C#, etc.) and a concrete scenario (e.g., making multiple HTTP requests, reading/writing files, processing streaming data). I can tailor the explanation and provide a ready-to-run example for that setup.\n",
      "\n",
      "==================================================\n",
      "BEGINNER USER\n",
      "==================================================\n",
      "Sure! Here’s a beginner-friendly way to understand async programming.\n",
      "\n",
      "What it is in simple words\n",
      "- Async programming lets your program start a long task (like a network request or reading a file) and then do other work instead of waiting idly.\n",
      "- When the long task finishes, your program handles the result. This helps apps stay responsive and use time efficiently.\n",
      "\n",
      "Two easy contrasts\n",
      "- Synchronous (blocking): If you do a long task, your program stops and waits until it finishes. Everything else waits too.\n",
      "- Asynchronous (non-blocking): Your program starts the long task, then moves on to other work. When the long task finishes, you get the result later.\n",
      "\n",
      "Key ideas behind async programming\n",
      "- Non-blocking I/O: Waiting for things like network responses or disk reads doesn’t hold everything up.\n",
      "- Concurrency vs. parallelism:\n",
      "  - Concurrency: Your program handles many tasks in a way that overlaps, not necessarily at the same exact moment.\n",
      "  - Parallelism: Tasks run at the same time (e.g., on multiple CPU cores). Async often helps with concurrency, especially for I/O tasks.\n",
      "- Event loop (the common mechanism in many languages): A loop that keeps track of tasks that are waiting and tasks that need to run, and it schedules them as soon as they’re ready.\n",
      "- Common patterns to organize async work:\n",
      "  - Callbacks: A function that runs when another task finishes.\n",
      "  - Promises/Futures: A placeholder for a value that will be available later.\n",
      "  - Async/await: A cleaner syntax to wait for a task to finish without getting tangled in callbacks.\n",
      "\n",
      "Simple examples (conceptual, with tiny code snippets)\n",
      "- JavaScript (web or Node.js)\n",
      "  - Callbacks (older style):\n",
      "    getData(function(result) {\n",
      "      console.log(result);\n",
      "    });\n",
      "  - Promises:\n",
      "    fetchData().then(result => console.log(result)).catch(err => console.error(err));\n",
      "  - Async/await (cleaner):\n",
      "    async function load() {\n",
      "      try {\n",
      "        const data = await fetchData();\n",
      "        console.log(data);\n",
      "      } catch (e) {\n",
      "        console.error(e);\n",
      "      }\n",
      "    }\n",
      "    load();\n",
      "\n",
      "- Python (asyncio)\n",
      "  - Async/await with asyncio:\n",
      "    import asyncio\n",
      "\n",
      "    async def fetch_data():\n",
      "        await asyncio.sleep(1)  # pretend we’re waiting for I/O\n",
      "        return \"data\"\n",
      "\n",
      "    async def main():\n",
      "        data = await fetch_data()\n",
      "        print(data)\n",
      "\n",
      "    asyncio.run(main())\n",
      "\n",
      "When async is especially useful\n",
      "- I/O-bound tasks: network requests, file reads/writes, databases.\n",
      "- Apps that should stay responsive while waiting for slow operations.\n",
      "\n",
      "When you might not need it (or need something else)\n",
      "- CPU-bound work: If you’re doing heavy calculations, async alone won’t speed things up. You might use threads or separate processes for true parallel work.\n",
      "\n",
      "Common gotchas to watch out for\n",
      "- Not awaiting or not handling results can leave you with “unfinished” tasks.\n",
      "- Errors can be harder to spot; use try/except around await or proper error handling in your async framework.\n",
      "- Shared data can cause race conditions if not careful. Keep state changes simple or use synchronization tools.\n",
      "\n",
      "A quick-start plan\n",
      "- Pick a language you’re using (JavaScript or Python are the most common for beginners).\n",
      "- Learn the basic pattern: start a task, do something else, then handle the result.\n",
      "- Try two small tasks in parallel (e.g., fetch two URLs at once or sleep two timers and then print).\n",
      "  - In JavaScript: use Promise.all([task1(), task2()]) to run in parallel.\n",
      "  - In Python: use asyncio.gather(task1(), task2()) to run in parallel.\n",
      "\n",
      "Want a tailored explanation with concrete code for your language (JavaScript, Python, or another)? Tell me which language you’re using and I’ll tailor examples and a tiny practice exercise.\n"
     ]
    }
   ],
   "source": [
    "# Expert user\n",
    "print(\"=\" * 50)\n",
    "print(\"EXPERT USER\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "result = agent_with_middleware.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Explain async programming\")]},\n",
    "    context={\"user_role\": \"expert\"}\n",
    ")\n",
    "print(result[\"messages\"][-1].content)\n",
    "print()\n",
    "\n",
    "# Beginner user\n",
    "print(\"=\" * 50)\n",
    "print(\"BEGINNER USER\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "result = agent_with_middleware.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Explain async programming\")]},\n",
    "    context={\"user_role\": \"beginner\"}\n",
    ")\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Custom Middleware - Request Logger\n",
    "\n",
    "Middleware lets you hook into the agent loop and see what's happening at each step. This is incredibly useful for debugging and understanding how your agent works.\n",
    "\n",
    "**The Agent Loop:**\n",
    "User Input --> [before_model] --> [wrap_model_call] --> Model --> [after_model] --> Tools --> ...\n",
    "\n",
    "**What we'll build:**\n",
    "A logger that prints information at each step:\n",
    "- **Before model** - How many messages are in the conversation?\n",
    "- **Wrap model call** - Which model and tools are being used?\n",
    "- **After model** - Did the model call a tool or give a final answer?\n",
    "\n",
    "This is like adding debug `print()` statements, but in a clean, reusable way!\n",
    "\n",
    "Let's create middleware that logs model requests for debugging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import AgentMiddleware, AgentState, ModelRequest, ModelResponse\n",
    "from typing import Any, Callable\n",
    "\n",
    "class RequestLoggerMiddleware(AgentMiddleware):\n",
    "    \"\"\"Logs all model requests for debugging.\"\"\"\n",
    "    \n",
    "    def before_model(self, state: AgentState, runtime) -> dict[str, Any] | None:\n",
    "        \"\"\"Log before model execution.\"\"\"\n",
    "        message_count = len(state.get(\"messages\", []))\n",
    "        print(f\"[BEFORE MODEL] Processing {message_count} messages\")\n",
    "        return None  # Don't modify state\n",
    "    \n",
    "    def wrap_model_call(\n",
    "        self, \n",
    "        request: ModelRequest,\n",
    "        handler: Callable[[ModelRequest], ModelResponse]\n",
    "    ) -> ModelResponse:\n",
    "        \"\"\"Log the model request details and call the handler.\"\"\"\n",
    "        print(f\"  [MODEL REQUEST]\")\n",
    "        print(f\"   Model: {request.model if hasattr(request, 'model') else 'default'}\")\n",
    "        print(f\"   Tools available: {len(request.tools) if request.tools else 0}\")\n",
    "        \n",
    "        # Call the actual model handler\n",
    "        return handler(request)\n",
    "    \n",
    "    def after_model(self, state: AgentState, runtime) -> dict[str, Any] | None:\n",
    "        \"\"\"Log after model execution.\"\"\"\n",
    "        last_message = state[\"messages\"][-1]\n",
    "        if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "            print(f\" [AFTER MODEL] Model requested {len(last_message.tool_calls)} tool call(s)\")\n",
    "        else:\n",
    "            print(f\" [AFTER MODEL] Model provided final response\")\n",
    "        return None  # Don't modify state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent with logger middleware\n",
    "agent_with_logger = create_agent(\n",
    "    model=\"openai:gpt-5-nano\",\n",
    "    tools=[explain_concept],\n",
    "    middleware=[RequestLoggerMiddleware()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to Expect\n",
    "\n",
    "When we run the agent with the logger, you'll see the execution flow in real-time:\n",
    "\n",
    "**First iteration:**\n",
    "1. `[BEFORE MODEL]` - Shows how many messages we're starting with\n",
    "2. `[MODEL REQUEST]` - Shows which model and tools are available (from wrap_model_call)\n",
    "3. `[AFTER MODEL]` - The model decides to call the `explain_concept` tool\n",
    "\n",
    "**Second iteration (after tool execution):**\n",
    "1. `[BEFORE MODEL]` - Now we have more messages (including tool result)\n",
    "2. `[MODEL REQUEST]` - Model info again\n",
    "3. `[AFTER MODEL]` - Model provides the final answer (no more tools needed)\n",
    "\n",
    "This gives you a detailed view into your agent's decision-making process.\n",
    "\n",
    "Let's run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "RUNNING AGENT WITH LOGGER\n",
      "==================================================\n",
      "\n",
      "[BEFORE MODEL] Processing 1 messages\n",
      "  [MODEL REQUEST]\n",
      "   Model: client=<openai.resources.chat.completions.completions.Completions object at 0x114fbcb00> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x114fbc3e0> root_client=<openai.OpenAI object at 0x114fbc2b0> root_async_client=<openai.AsyncOpenAI object at 0x114fbcc30> model_name='gpt-5-nano' model_kwargs={} openai_api_key=SecretStr('**********') stream_usage=True\n",
      "   Tools available: 1\n",
      " [AFTER MODEL] Model requested 1 tool call(s)\n",
      "[BEFORE MODEL] Processing 3 messages\n",
      "  [MODEL REQUEST]\n",
      "   Model: client=<openai.resources.chat.completions.completions.Completions object at 0x114fbcb00> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x114fbc3e0> root_client=<openai.OpenAI object at 0x114fbc2b0> root_async_client=<openai.AsyncOpenAI object at 0x114fbcc30> model_name='gpt-5-nano' model_kwargs={} openai_api_key=SecretStr('**********') stream_usage=True\n",
      "   Tools available: 1\n",
      " [AFTER MODEL] Model provided final response\n",
      "\n",
      "==================================================\n",
      "FINAL RESPONSE\n",
      "==================================================\n",
      "Recursion is a way of solving a problem by having a function call itself to solve a smaller version of the same problem, until it reaches a simple stopping condition.\n",
      "\n",
      "Key ideas:\n",
      "- Base case: a simple, non-recursive case that ends the recursion.\n",
      "- Recursive case: the function calls itself with a smaller or simpler input.\n",
      "- Each call adds a frame to the call stack; once a base case is reached, the calls unwind and produce the final result.\n",
      "\n",
      "Simple example: summing numbers from 1 to n\n",
      "- Idea: sum_to(n) = n + sum_to(n-1), with base case sum_to(0) = 0\n",
      "\n",
      "def sum_to(n):\n",
      "    if n <= 0:\n",
      "        return 0            # base case\n",
      "    else:\n",
      "        return n + sum_to(n-1)  # recursive case\n",
      "\n",
      "- How it works: sum_to(3) = 3 + sum_to(2) = 3 + (2 + sum_to(1)) = 3 + 2 + 1 + sum_to(0) = 6\n",
      "\n",
      "Common real-world patterns\n",
      "- Factorial:\n",
      "  - reason: factorial(n) = n * factorial(n-1), with factorial(0) = 1\n",
      "  - code:\n",
      "    def factorial(n):\n",
      "        if n <= 1:\n",
      "            return 1\n",
      "        return n * factorial(n-1)\n",
      "- Binary search on a sorted array:\n",
      "  - reason: divide the problem in half each step\n",
      "  - code (conceptual):\n",
      "    def binary_search(arr, target, lo=0, hi=None):\n",
      "        if hi is None: hi = len(arr) - 1\n",
      "        if lo > hi: return -1  # not found\n",
      "        mid = (lo + hi) // 2\n",
      "        if arr[mid] == target: return mid\n",
      "        if arr[mid] < target:\n",
      "            return binary_search(arr, target, mid + 1, hi)\n",
      "        else:\n",
      "            return binary_search(arr, target, lo, mid - 1)\n",
      "\n",
      "Things to watch out for\n",
      "- Base case must exist and be correct; missing or incorrect base case can lead to infinite recursion and eventually a crash.\n",
      "- Depth limits: in many languages, deep recursion can exhaust the call stack (Python, for example, has a recursion limit around 1000 by default).\n",
      "- Performance: each call adds overhead; some problems are better solved iteratively or with dynamic programming (memoization) to avoid repeated work.\n",
      "\n",
      "When to use recursion vs iteration\n",
      "- Use recursion when the problem has a natural self-similar structure (trees, graphs, divide-and-conquer, mathematical definitions).\n",
      "- If depth could be large or performance is critical, consider an iterative approach or optimize with techniques like tail recursion (where the recursive call is the last operation). Note that many languages don’t optimize tail recursion automatically (Python, for example), so you still get a stack buildup.\n",
      "\n",
      "Recursion is a powerful concept, but it’s important to ensure a correct base case, reasonable depth, and to consider alternatives if necessary for performance or language limitations. If you want, I can tailor an example to a specific problem you’re working on.\n"
     ]
    }
   ],
   "source": [
    "# Run and observe the logs\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"RUNNING AGENT WITH LOGGER\")\n",
    "print(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "result = agent_with_logger.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Explain recursion\"}]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"FINAL RESPONSE\")\n",
    "print(\"=\" * 50)\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Combining Middleware and Human-in-the-loop\n",
    "\n",
    "Let's combine human-in-the-loop AND middleware for a production-ready agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensitive tool that needs approval\n",
    "@tool\n",
    "def delete_database(database_name: str) -> str:\n",
    "    \"\"\"Delete a database. THIS IS DANGEROUS!\"\"\"\n",
    "    \n",
    "    response = interrupt({\n",
    "        \"action\": \"delete_database\",\n",
    "        \"database_name\": database_name,\n",
    "        \"warning\": \"This will permanently delete the database!\",\n",
    "        \"message\": \"Are you absolutely sure?\"\n",
    "    })\n",
    "    \n",
    "    if response.get(\"confirmed\"):\n",
    "        return f\"Database '{database_name}' has been deleted (simulation)\"\n",
    "    else:\n",
    "        return \"Database deletion cancelled\"\n",
    "\n",
    "# Middleware to track dangerous operations\n",
    "class SafetyMiddleware(AgentMiddleware):\n",
    "    \"\"\"Add safety checks and logging.\"\"\"\n",
    "    \n",
    "    name = \"safety_checker\"\n",
    "    \n",
    "    def after_model(self, state: AgentState) -> dict[str, Any] | None:\n",
    "        \"\"\"Check for dangerous tool calls.\"\"\"\n",
    "        last_message = state[\"messages\"][-1]\n",
    "        \n",
    "        if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "            for tool_call in last_message.tool_calls:\n",
    "                if \"delete\" in tool_call[\"name\"].lower():\n",
    "                    print(\"   [SAFETY] Dangerous operation detected!\")\n",
    "                    print(f\"   Tool: {tool_call['name']}\")\n",
    "                    print(f\"   Args: {tool_call['args']}\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "# Create production agent\n",
    "production_agent = create_agent(\n",
    "    model=\"openai:gpt-5-nano\",\n",
    "    tools=[delete_database],\n",
    "    middleware=[SafetyMiddleware()],\n",
    "    checkpointer=MemorySaver()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### What to Expect: Layered Safety in Action\n",
    "\n",
    "  When we attempt a dangerous operation, you'll see **both** safety mechanisms activate:\n",
    "\n",
    "  **Layer 1 - Middleware Detection:**\n",
    "  - `[SAFETY] Dangerous operation detected!` - Middleware spots the delete operation\n",
    "  - Logs the tool name and arguments for audit trails\n",
    "\n",
    "  **Layer 2 - Human Approval (Interrupt):**\n",
    "  - Agent execution pauses at the `interrupt()`\n",
    "  - Warning message displayed to human reviewer\n",
    "  - Execution won't continue until explicit approval\n",
    "\n",
    "  **This is defense-in-depth:** Middleware monitors ALL operations, while interrupts enforce human approval for critical actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "DANGEROUS OPERATION ATTEMPT\n",
      "==================================================\n",
      "\n",
      "\n",
      "(In a real app, a human would review this before proceeding)\n"
     ]
    }
   ],
   "source": [
    "# Test the combined pattern\n",
    "thread_id_4 = str(uuid.uuid4())\n",
    "config_4 = {\"configurable\": {\"thread_id\": thread_id_4}}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"DANGEROUS OPERATION ATTEMPT\")\n",
    "print(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "# Run until interrupt\n",
    "result = production_agent.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"Delete the production_db database\")]\n",
    "    },\n",
    "    config=config_4\n",
    ")\n",
    "\n",
    "if \"__interrupt__\" in result:\n",
    "    interrupt_info = result[\"__interrupt__\"][0]\n",
    "    print(\"\\n  Human approval required:\")\n",
    "    print(f\"   {interrupt_info.value['warning']}\")\n",
    "    print(f\"   Database: {interrupt_info.value['database_name']}\")\n",
    "\n",
    "print(\"\\n(In a real app, a human would review this before proceeding)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Human-in-the-Loop (Interrupts)\n",
    "- Use `interrupt()` to pause execution\n",
    "- Requires a `checkpointer` for persistence\n",
    "- Resume with `Command(resume=value)`\n",
    "- Perfect for approval workflows and sensitive operations\n",
    "\n",
    "### Middleware\n",
    "- **Node-style hooks**: `before_model`, `after_model` - Sequential logic, validation, logging\n",
    "- **Wrap-style hooks**: `wrap_model_call`, `wrap_tool_call` - Full control, retries, transformation\n",
    "- **Decorators**: `@dynamic_prompt`, `@before_model`, `@wrap_model_call` for quick middleware\n",
    "- **Classes**: Subclass `AgentMiddleware` for complex, reusable components\n",
    "\n",
    "### When to Use What?\n",
    "\n",
    "**Use Interrupts when:**\n",
    "- You need human approval for actions\n",
    "- You want to review/edit tool calls\n",
    "- You need to validate user input\n",
    "\n",
    "**Use Middleware when:**\n",
    "- You need to modify agent behavior dynamically\n",
    "- You want to add logging/monitoring\n",
    "- You need to enforce policies (token limits, safety checks)\n",
    "- You want to personalize responses based on context\n",
    "\n",
    "**Node-style vs Wrap-style:**\n",
    "- Node-style for sequential operations (logging, validation)\n",
    "- Wrap-style for control flow (retry, fallback, caching)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercise (Optional)\n",
    "\n",
    "Try building an agent that:\n",
    "1. Has a tool to make a purchase\n",
    "2. Uses middleware to check if the purchase amount is over $1000\n",
    "3. If over $1000, uses an interrupt to require approval\n",
    "4. If under $1000, processes automatically\n",
    "\n",
    "Hint: Combine `before_model` middleware with conditional `interrupt()` logic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "# Challenge: Build the purchase approval agent\n",
    "\n",
    "# @tool\n",
    "# def make_purchase(item: str, amount: float) -> str:\n",
    "#     ...\n",
    "\n",
    "# class PurchaseLimitMiddleware(AgentMiddleware):\n",
    "#     ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "You now have powerful tools for building production agents!\n",
    "\n",
    "**Continue your journey:**\n",
    "1.  Check out `multi_agent.ipynb` for multi-agent systems\n",
    "2.  Explore built-in middleware (Summarization, Anthropic Prompt Caching)\n",
    "3.  Build your own custom middleware for your use case\n",
    "4.  Add LangSmith for debugging and monitoring\n",
    "\n",
    "**Resources:**\n",
    "- [Middleware Documentation](https://docs.langchain.com/oss/python/langchain/middleware)\n",
    "- [Human-in-the-Loop Guide](https://docs.langchain.com/oss/python/langchain/human-in-the-loop)\n",
    "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)\n",
    "\n",
    "</br>\n",
    "</br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
